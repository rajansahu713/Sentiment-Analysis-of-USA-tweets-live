{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis model using twitter data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "opvxsjwFP5TF",
        "outputId": "92b48b77-18f2-4a8e-c24d-da6d997922f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akcputIqlTSm",
        "outputId": "46d155b3-0661-4d51-ac59-2550b6e8edd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH7QXmWuTrUE",
        "outputId": "6b579376-0b96-4902-cb21-37340a92547c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Text Classifiation using NLP\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcKfNeLfP8UZ",
        "outputId": "0ec2a7aa-7d2a-45b4-d4c1-7be995f63a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Importing the dataset\n",
        "reviews = load_files('/content/drive/My Drive/future/NLP/Dataset/txt_sentoken/')\n",
        "X,y = reviews.data,reviews.target\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEZ7mZjnQ5Ew"
      },
      "source": [
        "# Pickling the dataset\n",
        "with open('/content/drive/My Drive/future/NLP/Model/X.pickle','wb') as f:\n",
        "    pickle.dump(X,f)\n",
        "    \n",
        "with open('/content/drive/My Drive/future/NLP/Model/y.pickle','wb') as f:\n",
        "    pickle.dump(y,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YffjwFV_SMuf"
      },
      "source": [
        "# Unpickling dataset\n",
        "X_in = open('/content/drive/My Drive/future/NLP/Model/X.pickle','rb')\n",
        "y_in = open('/content/drive/My Drive/future/NLP/Model/y.pickle','rb')\n",
        "X = pickle.load(X_in)\n",
        "y = pickle.load(y_in)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyYk4fyFRE2F"
      },
      "source": [
        "# Creating the corpus\n",
        "corpus = []\n",
        "for i in range(0, 2000):\n",
        "    review = re.sub(r'\\W', ' ', str(X[i]))\n",
        "    review = review.lower()\n",
        "    review = re.sub(r'^br$', ' ', review)\n",
        "    review = re.sub(r'\\s+br\\s+',' ',review)\n",
        "    review = re.sub(r'\\s+[a-z]\\s+', ' ',review)\n",
        "    review = re.sub(r'^b\\s+', '', review)\n",
        "    review = re.sub(r'\\s+', ' ', review)\n",
        "    corpus.append(review)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHRTD-M5RNqe"
      },
      "source": [
        "# Creating the BOW model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(corpus).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO5INBAqRYR6"
      },
      "source": [
        "# Creating the Tf-Idf Model\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "transformer = TfidfTransformer()\n",
        "X = transformer.fit_transform(X).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5kuS_gRRfiw",
        "outputId": "0bc4e281-0765-49f4-fec9-2cbf60cc9681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating the Tf-Idf model directly\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.06887219 0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.12007883 0.         0.06321361 ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU_hyX5bRno8"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLywSljfR1VY",
        "outputId": "82edf534-8bf3-4b62-c245-144fc00980a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Training the classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(text_train,sent_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do-FwIF3R8bP"
      },
      "source": [
        "# Testing model performance\n",
        "sent_pred = classifier.predict(text_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(sent_test, sent_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpgV7N2ETXJB"
      },
      "source": [
        "# Saving our classifier\n",
        "with open('/content/drive/My Drive/future/NLP/Model/classifier.pickle','wb') as f:\n",
        "    pickle.dump(classifier,f)\n",
        "    \n",
        "# Saving the Tf-Idf model\n",
        "with open('/content/drive/My Drive/future/NLP/Model/tfidfmodel.pickle','wb') as f:\n",
        "    pickle.dump(vectorizer,f)\n",
        "    \n",
        "\n",
        "# Using our classifier\n",
        "with open('/content/drive/My Drive/future/NLP/Model/tfidfmodel.pickle','rb') as f:\n",
        "    tfidf = pickle.load(f)\n",
        "    \n",
        "with open('/content/drive/My Drive/future/NLP/Model/classifier.pickle','rb') as f:\n",
        "    clf = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td_lgJ5FTko4"
      },
      "source": [
        "sample = [\"You are a nice person man, have a good life\"]\n",
        "sample = tfidf.transform(sample).toarray()\n",
        "sentiment = clf.predict(sample)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}